{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### **Imports and setup** \n",
    "Import packages, the config.py and architecture.py files and set global parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from config import (\n",
    "    LEARNING_RATE,\n",
    "    NUM_EPOCHS,\n",
    "    TRAIN_SIZE,\n",
    "    FP_BITS,\n",
    "    FP_RADIUS,\n",
    "    list_node_features,\n",
    "    list_edge_features,\n",
    "    BATCH_SIZE,\n",
    "    NUM_ESTIMATORS,\n",
    "    num_node_features,\n",
    "    num_edge_features\n",
    ")\n",
    "from pkasolver.constants import SEED, DEVICE\n",
    "from pkasolver.data import (\n",
    "    load_data,\n",
    "    preprocess_all,\n",
    "    train_validation_set_split,\n",
    "    make_stat_variables,\n",
    "    make_pyg_dataset_from_dataframe,\n",
    "    slice_list,\n",
    "    cross_val_lists,\n",
    "    \n",
    ")\n",
    "from pkasolver.ml_architecture import (\n",
    "    GCN_pair,\n",
    "    GCN_prot,\n",
    "    GCN_deprot,\n",
    "    NNConv_pair,\n",
    "    NNConv_deprot,\n",
    "    NNConv_prot,\n",
    "    gcn_full_training,\n",
    ")\n",
    "from pkasolver.chem import generate_morgan_fp_array, calculate_tanimoto_coefficient\n",
    "from pkasolver.ml import dataset_to_dataloader, test_ml_model, test_graph_model\n",
    "from pkasolver.stat import compute_stats\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (6.25, 6.25)\n",
    "sns.set_theme(style=\"ticks\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from IPython.display import display\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from rdkit.Chem import Draw\n",
    "\n",
    "random.seed(SEED)\n",
    "imgdir = \"images_and_tables\"\n",
    "os.makedirs(imgdir, exist_ok=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Setting num threads to 1\n",
      "Pytorch will use cuda\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "torch.cuda.memory_stats(device='cuda')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "OrderedDict()"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "! nvidia-smi"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wed Sep 29 17:07:19 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:82:00.0 Off |                  N/A |\n",
      "|  0%   30C    P8     9W / 250W |    609MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A       737      C   python                            454MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Data Preprocessing**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **Import raw data**\n",
    "Load data from sdf files, create conjugate molescules and store them in pandas DataFrames"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# make sure the directory for saving the run data exists\n",
    "run_filename = \"run_data\"\n",
    "os.makedirs(f\"{run_filename}/\", exist_ok=True)\n",
    "\n",
    "# check if saved dictonary of Dataframes is available and if so, import it\n",
    "if os.path.isfile(f\"{run_filename}/data_dfs.pkl\"):\n",
    "    print(\"Loading data ...\")\n",
    "    with open(f\"{run_filename}/data_dfs.pkl\", \"rb\") as pickle_file:\n",
    "        dataset = pickle.load(pickle_file)\n",
    "\n",
    "# create DataFrames for each dataset, store them in a dictonary and save it as as .pkl file in in the run_data folder\n",
    "else:\n",
    "    print(\"Generating data ...\")\n",
    "    sdf_paths = load_data(\"../../data/Baltruschat/\")\n",
    "    dataset = preprocess_all(sdf_paths)\n",
    "    dataset[\"train_split\"], dataset[\"val_split\"] = train_validation_set_split(\n",
    "        dataset[\"Training\"], TRAIN_SIZE, SEED\n",
    "    )\n",
    "\n",
    "    with open(f\"{run_filename}/data_dfs.pkl\", \"wb\") as pickle_file:\n",
    "        pickle.dump(dataset, pickle_file)\n",
    "\n",
    "# notification\n",
    "print(dataset.keys())\n",
    "display(dataset[\"train_split\"].head(1))\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading data ...\n",
      "dict_keys(['Training', 'Novartis', 'Literature', 'train_split', 'val_split'])\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pKa</th>\n",
       "      <th>marvin_pKa</th>\n",
       "      <th>marvin_atom</th>\n",
       "      <th>marvin_pKa_type</th>\n",
       "      <th>original_dataset</th>\n",
       "      <th>ID</th>\n",
       "      <th>smiles</th>\n",
       "      <th>protonated</th>\n",
       "      <th>deprotonated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>9.7</td>\n",
       "      <td>9.63</td>\n",
       "      <td>4</td>\n",
       "      <td>basic</td>\n",
       "      <td>['chembl25']</td>\n",
       "      <td>871123</td>\n",
       "      <td>CC(C)(C)[NH2+]CC(O)c1cc(Cl)c(N)c(C(F)(F)F)c1</td>\n",
       "      <td><img data-content=\"rdkit/molecule\" src=\"data:image/png;base64,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\" alt=\"Mol\"/></td>\n",
       "      <td><img data-content=\"rdkit/molecule\" src=\"data:image/png;base64,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\" alt=\"Mol\"/></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     pKa marvin_pKa marvin_atom marvin_pKa_type original_dataset      ID  \\\n",
       "536  9.7       9.63           4           basic     ['chembl25']  871123   \n",
       "\n",
       "                                           smiles  \\\n",
       "536  CC(C)(C)[NH2+]CC(O)c1cc(Cl)c(N)c(C(F)(F)F)c1   \n",
       "\n",
       "                                            protonated  \\\n",
       "536  <img data-content=\"rdkit/molecule\" src=\"data:i...   \n",
       "\n",
       "                                          deprotonated  \n",
       "536  <img data-content=\"rdkit/molecule\" src=\"data:i...  "
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Calulate fingerprint based data**\n",
    "Create Fingerprints, target-value objects and add best tanimoto similarities of fps form external validation set molecules with those of the train molecules"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "\n",
    "\n",
    "# check if saved dictonary of fingerprint data is available and if so, import it\n",
    "if os.path.isfile(f\"{run_filename}/fp_data.pkl\"):\n",
    "    with open(f\"{run_filename}/fp_data.pkl\", \"rb\") as pickle_file:\n",
    "        fp_data = pickle.load(pickle_file)\n",
    "\n",
    "# create fingerprint arrays (dim: num molecules x fp bits) for each dataset, store them in a dictonary and save it as as .pkl file in in the run_data folder\n",
    "else:\n",
    "    fp_data = {}\n",
    "    for name, df in dataset.items():\n",
    "        X_feat, y = make_stat_variables(df, [], [\"pKa\"])\n",
    "        X_prot = generate_morgan_fp_array(\n",
    "            df, \"protonated\", nBits=FP_BITS, radius=FP_RADIUS, useFeatures=True\n",
    "        )\n",
    "        X_deprot = generate_morgan_fp_array(\n",
    "            df, \"deprotonated\", nBits=FP_BITS, radius=FP_RADIUS, useFeatures=True\n",
    "        )\n",
    "        X = np.concatenate((X_prot, X_deprot), axis=1)\n",
    "        fp_data[f\"{name}\"] = {\"prot\": X_prot, \"deprot\": X_deprot, \"pair\": X, \"y\": y}\n",
    "\n",
    "    with open(f\"{run_filename}/fp_data.pkl\", \"wb\") as f:\n",
    "        pickle.dump(fp_data, f)\n",
    "\n",
    "    # add max tanimotosimilarity to the Dataframes of external test sets\n",
    "    train_name = \"train_split\"\n",
    "    val_name = \"val_split\"\n",
    "    for name, dataset in fp_data.items():\n",
    "        if name in [train_name, val_name]:\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"calculating similarities for {name}\")\n",
    "            max_scores = []\n",
    "            for test_mol in dataset[\"prot\"]:\n",
    "                scores = []\n",
    "                for ref_mol in fp_data[train_name][\"prot\"]:\n",
    "                    scores.append(calculate_tanimoto_coefficient(test_mol, ref_mol))\n",
    "                max_scores.append(max(scores))\n",
    "            dataset[name][\"Similarity_max\"] = max_scores\n",
    "\n",
    "    with open(f\"{run_filename}/data_dfs.pkl\", \"wb\") as f:\n",
    "        pickle.dump(dataset, f)\n",
    "\n",
    "# notification\n",
    "print(\"fp_data keys:\", fp_data.keys())\n",
    "print(f\"calculated/loaded fingerprint data successfully\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "fp_data keys: dict_keys(['Training', 'Novartis', 'Literture', 'train_split', 'val_split'])\n",
      "calculated/loaded fingerprint data successfully\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Calculate graph data**\n",
    "Create graph data with node and edge features specified in the config file and prepare loaders"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# check if saved dictonary of graph data is available and if so, import it\n",
    "if os.path.isfile(f\"{run_filename}/graph_data.pkl\"):\n",
    "    print(\"Loading data ...\")\n",
    "    with open(f\"{run_filename}/graph_data.pkl\", \"rb\") as f:\n",
    "        graph_data = pickle.load(f)\n",
    "\n",
    "# create list of 'PairData' graph objects for each dataset, store them in a dictonary and save it as as .pkl file in in the run_data folder\n",
    "else:\n",
    "    print(\"Generating data ...\")\n",
    "    graph_data = {}\n",
    "    for name, df in dataset.items():\n",
    "        print(f\"Generating data for: {name}\")\n",
    "        graph_data[name] = make_pyg_dataset_from_dataframe(\n",
    "            df, list_node_features, list_edge_features, paired=True\n",
    "        )\n",
    "    with open(f\"{run_filename}/graph_data.pkl\", \"wb\") as f:\n",
    "        pickle.dump(graph_data, f)\n",
    "\n",
    "print(\"graph_data keys:\", graph_data.keys())\n",
    "\n",
    "# create an iterable loader object from the list of graph data of each dataset and store them in a dictonary\n",
    "loaders = {}\n",
    "for name, dataset in graph_data.items():\n",
    "    print(f\"Generating loader for {name}\")\n",
    "    if name == \"Training\":\n",
    "        print(\"Skipping unsplit Training dataset\")\n",
    "        continue\n",
    "    loaders[name] = dataset_to_dataloader(dataset, BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# notification\n",
    "print(\"loaders keys:\", loaders.keys())\n",
    "print(f\"calculated/loaded graph data successfully\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading data ...\n",
      "graph_data keys: dict_keys(['Training', 'Novartis', 'Literature', 'train_split', 'val_split'])\n",
      "Generating loader for Training\n",
      "Skipping unsplit Training dataset\n",
      "Generating loader for Novartis\n",
      "Generating loader for Literature\n",
      "Generating loader for train_split\n",
      "Generating loader for val_split\n",
      "loaders keys: dict_keys(['Novartis', 'Literature', 'train_split', 'val_split'])\n",
      "calculated/loaded graph data successfully\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**show feature value range**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# print all possible node feature values\n",
    "values = [set() for i in range(len(list_node_features))]\n",
    "for dataset in graph_data.values():\n",
    "    for entry in dataset:\n",
    "        for i, row in enumerate(entry.x_p.cpu().T):\n",
    "            values[i] = values[i] | set(row.numpy())\n",
    "        for i, row in enumerate(entry.x_d.cpu().T):\n",
    "            values[i] = values[i] | set(row.numpy())\n",
    "print(\"Node features:\")\n",
    "for name, values in zip(list_node_features, values):\n",
    "    x = list(values)\n",
    "    x.sort()\n",
    "    print(f\"{name}:{x}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# print all possible edge feature values\n",
    "values = [set() for i in range(len(list_edge_features))]\n",
    "for dataset in graph_data.values():\n",
    "    for entry in dataset:\n",
    "        for i, row in enumerate(entry.edge_attr_p.cpu().T):\n",
    "            values[i] = values[i] | set(row.numpy())\n",
    "        for i, row in enumerate(entry.edge_attr_d.cpu().T):\n",
    "            values[i] = values[i] | set(row.numpy())\n",
    "print(\"Edge features:\")\n",
    "for name, values in zip(list_edge_features, values):\n",
    "    x = list(values)\n",
    "    x.sort()\n",
    "    print(f\"{name}:{x}\")\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Node features:\n",
      "atomic_number:[1.0, 6.0, 7.0, 8.0, 9.0, 15.0, 16.0, 17.0, 33.0, 35.0, 53.0]\n",
      "formal_charge:[-1.0, 0.0, 1.0]\n",
      "hybridization:[1.0, 2.0, 3.0, 4.0]\n",
      "total_num_Hs:[0.0, 1.0, 2.0, 3.0]\n",
      "aromatic_tag:[0.0, 1.0]\n",
      "total_valence:[1.0, 2.0, 3.0, 4.0, 5.0, 6.0]\n",
      "total_degree:[1.0, 2.0, 3.0, 4.0]\n",
      "is_in_ring:[0.0, 1.0]\n",
      "\n",
      "\n",
      "Edge features:\n",
      "bond_type:[1.0, 1.5, 2.0, 3.0]\n",
      "is_conjugated:[0.0, 1.0]\n",
      "rotatable:[0.0, 1.0]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Training of predictive models**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **train baseline models**\n",
    "train all baseline models in protonated, deprotonated and pair mode"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "models_dict = {\n",
    "    \"RFR\": RandomForestRegressor(\n",
    "        n_estimators=NUM_ESTIMATORS, random_state=SEED\n",
    "    ),  # Baltruschat n_estimatores = 1000\n",
    "    \"PLS\": PLSRegression(),\n",
    "}\n",
    "\n",
    "baseline_models = {}\n",
    "train_name = \"train_split\"\n",
    "val_name = \"val_split\"\n",
    "\n",
    "for model_name, model_template in models_dict.items():\n",
    "    baseline_models[model_name] = {}\n",
    "    for mode, X in fp_data[train_name].items():\n",
    "        if mode == \"y\":\n",
    "            continue\n",
    "        path = f\"models/baseline/{model_name}/{mode}/\"\n",
    "        if os.path.isfile(path + \"model.pkl\"):\n",
    "            with open(path + \"model.pkl\", \"rb\") as pickle_file:\n",
    "                baseline_models[model_name][mode] = pickle.load(pickle_file)\n",
    "        else:\n",
    "            y = fp_data[train_name][\"y\"]\n",
    "            y_val = fp_data[val_name][\"y\"]\n",
    "            model = copy.deepcopy(model_template)\n",
    "            model.fit(X, y)\n",
    "            print(f\"{model_name}_{mode}: {model.score(fp_data[val_name][mode], y_val)}\")\n",
    "            baseline_models[model_name][mode] = model\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            with open(path + \"model.pkl\", \"wb\") as pickle_file:\n",
    "                pickle.dump(model, pickle_file)\n",
    "print(f\"trained/loaded baseline models successfully\")\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "trained/loaded baseline models successfully\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/data/shared/projects/pKa-prediction/anaconda3/envs/pkasolver/lib/python3.9/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator DecisionTreeRegressor from version 0.23.2 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "/data/shared/projects/pKa-prediction/anaconda3/envs/pkasolver/lib/python3.9/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator RandomForestRegressor from version 0.23.2 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "/data/shared/projects/pKa-prediction/anaconda3/envs/pkasolver/lib/python3.9/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator PLSRegression from version 0.23.2 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Train graph models**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Training**\n",
    "train all graph models in protonated, deprotonated and pair mode"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "embedding_size = 96\n",
    "num_graph_layer = 4\n",
    "num_linear_layer = 2\n",
    "\n",
    "gcn_dict = {\n",
    "    \"prot\": {\"no-edge\": GCN_prot, \"edge\": NNConv_prot},\n",
    "    \"deprot\": {\"no-edge\": GCN_deprot, \"edge\": NNConv_deprot},\n",
    "    \"pair\": {\"no-edge\": GCN_pair, \"edge\": NNConv_pair},\n",
    "}\n",
    "\n",
    "mol_modes = [\"prot\", \"deprot\", \"pair\"]\n",
    "edge_modes = [\"no-edge\", \"edge\"]\n",
    "\n",
    "graph_models = {}\n",
    "for mode in mol_modes:\n",
    "    graph_models[mode] = {}\n",
    "    for edge in edge_modes:\n",
    "        path = f\"models/gcn/{mode}/{edge}/\"\n",
    "\n",
    "        if os.path.isfile(path + \"model.pkl\"):\n",
    "            with open(path + \"model.pkl\", \"rb\") as pickle_file:\n",
    "                graph_models[mode][edge] = pickle.load(pickle_file)\n",
    "            model = graph_models[mode][edge]\n",
    "        else:\n",
    "            model = gcn_dict[mode][edge](96, 4, 2, num_node_features, num_edge_features)\n",
    "            graph_models[mode][edge] = model\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "\n",
    "        if model.checkpoint[\"epoch\"] < NUM_EPOCHS:\n",
    "            print(model.checkpoint[\"epoch\"])\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "            print(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))            \n",
    "            try:\n",
    "                optimizer.load_state_dict(model.checkpoint[\"optimizer_state\"])\n",
    "            except:\n",
    "                pass\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, patience=5, verbose=True\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                f'Training GCN_{mode} with {edge} at epoch {model.checkpoint[\"epoch\"]}...'\n",
    "            )\n",
    "            print(model)\n",
    "            print(f'Training on {DEVICE}.')\n",
    "            gcn_full_training(\n",
    "                model,\n",
    "                loaders[\"train_split\"],\n",
    "                loaders[\"val_split\"],\n",
    "                optimizer,\n",
    "                path,\n",
    "                NUM_EPOCHS,\n",
    "            )\n",
    "\n",
    "            with open(path + \"model.pkl\", \"wb\") as pickle_file:\n",
    "                pickle.dump(model.to(device='cpu'), pickle_file)\n",
    "print(f\"trained/loaded gcn models successfully\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "Number of parameters:  38209\n",
      "Training GCN_prot with no-edge at epoch 0...\n",
      "GCN_prot(\n",
      "  (convs): ModuleList(\n",
      "    (0): GCNConv(8, 96)\n",
      "    (1): GCNConv(96, 96)\n",
      "    (2): GCNConv(96, 96)\n",
      "    (3): GCNConv(96, 96)\n",
      "  )\n",
      "  (lin): ModuleList(\n",
      "    (0): Linear(in_features=96, out_features=96, bias=True)\n",
      "    (1): Linear(in_features=96, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Training on cuda.\n",
      "Epoch: 000, Train MAE: 6.7820, Test MAE: 6.9170\n",
      "Epoch: 020, Train MAE: 1.7300, Test MAE: 1.6990\n",
      "Epoch: 040, Train MAE: 1.2260, Test MAE: 1.2170\n",
      "Epoch: 060, Train MAE: 1.0930, Test MAE: 1.1610\n",
      "Epoch: 080, Train MAE: 0.9780, Test MAE: 1.0370\n",
      "Epoch: 100, Train MAE: 0.9270, Test MAE: 0.9750\n",
      "0\n",
      "Number of parameters:  208065\n",
      "Training GCN_prot with edge at epoch 0...\n",
      "NNConv_prot(\n",
      "  (convs): ModuleList(\n",
      "    (0): NNConv(8, 96, aggr=\"add\", nn=Sequential(\n",
      "      (0): Linear(in_features=3, out_features=16, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=16, out_features=768, bias=True)\n",
      "    ))\n",
      "    (1): NNConv(96, 96, aggr=\"add\", nn=Sequential(\n",
      "      (0): Linear(in_features=3, out_features=16, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=16, out_features=9216, bias=True)\n",
      "    ))\n",
      "    (2): NNConv(96, 96, aggr=\"add\", nn=Sequential(\n",
      "      (0): Linear(in_features=3, out_features=16, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=16, out_features=9216, bias=True)\n",
      "    ))\n",
      "    (3): NNConv(96, 96, aggr=\"add\", nn=Sequential(\n",
      "      (0): Linear(in_features=3, out_features=16, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=16, out_features=9216, bias=True)\n",
      "    ))\n",
      "  )\n",
      "  (lin): ModuleList(\n",
      "    (0): Linear(in_features=96, out_features=96, bias=True)\n",
      "    (1): Linear(in_features=96, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Training on cuda.\n",
      "Epoch: 000, Train MAE: 167.7220, Test MAE: 173.1120\n",
      "Epoch: 020, Train MAE: 1.7320, Test MAE: 1.7860\n",
      "Epoch: 040, Train MAE: 1.4950, Test MAE: 1.5660\n",
      "Epoch: 060, Train MAE: 1.3800, Test MAE: 1.4250\n",
      "Epoch: 080, Train MAE: 1.4190, Test MAE: 1.5270\n",
      "Epoch: 100, Train MAE: 1.0650, Test MAE: 1.1790\n",
      "0\n",
      "Number of parameters:  38209\n",
      "Training GCN_deprot with no-edge at epoch 0...\n",
      "GCN_deprot(\n",
      "  (convs): ModuleList(\n",
      "    (0): GCNConv(8, 96)\n",
      "    (1): GCNConv(96, 96)\n",
      "    (2): GCNConv(96, 96)\n",
      "    (3): GCNConv(96, 96)\n",
      "  )\n",
      "  (lin): ModuleList(\n",
      "    (0): Linear(in_features=96, out_features=96, bias=True)\n",
      "    (1): Linear(in_features=96, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Training on cuda.\n",
      "Epoch: 000, Train MAE: 6.7810, Test MAE: 6.9170\n",
      "Epoch: 020, Train MAE: 1.5430, Test MAE: 1.5620\n",
      "Epoch: 040, Train MAE: 1.1340, Test MAE: 1.1400\n",
      "Epoch: 060, Train MAE: 1.0950, Test MAE: 1.1250\n",
      "Epoch: 080, Train MAE: 0.9820, Test MAE: 1.0220\n",
      "Epoch: 100, Train MAE: 0.9180, Test MAE: 0.9530\n",
      "0\n",
      "Number of parameters:  208065\n",
      "Training GCN_deprot with edge at epoch 0...\n",
      "NNConv_deprot(\n",
      "  (convs): ModuleList(\n",
      "    (0): NNConv(8, 96, aggr=\"add\", nn=Sequential(\n",
      "      (0): Linear(in_features=3, out_features=16, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=16, out_features=768, bias=True)\n",
      "    ))\n",
      "    (1): NNConv(96, 96, aggr=\"add\", nn=Sequential(\n",
      "      (0): Linear(in_features=3, out_features=16, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=16, out_features=9216, bias=True)\n",
      "    ))\n",
      "    (2): NNConv(96, 96, aggr=\"add\", nn=Sequential(\n",
      "      (0): Linear(in_features=3, out_features=16, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=16, out_features=9216, bias=True)\n",
      "    ))\n",
      "    (3): NNConv(96, 96, aggr=\"add\", nn=Sequential(\n",
      "      (0): Linear(in_features=3, out_features=16, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=16, out_features=9216, bias=True)\n",
      "    ))\n",
      "  )\n",
      "  (lin): ModuleList(\n",
      "    (0): Linear(in_features=96, out_features=96, bias=True)\n",
      "    (1): Linear(in_features=96, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Training on cuda.\n",
      "Epoch: 000, Train MAE: 169.1580, Test MAE: 174.6300\n",
      "Epoch: 020, Train MAE: 1.7560, Test MAE: 1.7990\n",
      "Epoch: 040, Train MAE: 1.5160, Test MAE: 1.5980\n",
      "Epoch: 060, Train MAE: 1.3610, Test MAE: 1.4650\n",
      "Epoch: 080, Train MAE: 1.4740, Test MAE: 1.5730\n",
      "Epoch: 100, Train MAE: 1.1320, Test MAE: 1.2030\n",
      "0\n",
      "Number of parameters:  94849\n",
      "Training GCN_pair with no-edge at epoch 0...\n",
      "GCN_pair(\n",
      "  (convs_p): ModuleList(\n",
      "    (0): GCNConv(8, 96)\n",
      "    (1): GCNConv(96, 96)\n",
      "    (2): GCNConv(96, 96)\n",
      "    (3): GCNConv(96, 96)\n",
      "  )\n",
      "  (convs_d): ModuleList(\n",
      "    (0): GCNConv(8, 96)\n",
      "    (1): GCNConv(96, 96)\n",
      "    (2): GCNConv(96, 96)\n",
      "    (3): GCNConv(96, 96)\n",
      "  )\n",
      "  (lin): ModuleList(\n",
      "    (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "    (1): Linear(in_features=192, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Training on cuda.\n",
      "Epoch: 000, Train MAE: 6.8720, Test MAE: 7.0030\n",
      "Epoch: 020, Train MAE: 1.5070, Test MAE: 1.5230\n",
      "Epoch: 040, Train MAE: 1.1830, Test MAE: 1.1820\n",
      "Epoch: 060, Train MAE: 0.9300, Test MAE: 0.9410\n",
      "Epoch: 080, Train MAE: 0.8780, Test MAE: 0.9190\n",
      "Epoch: 100, Train MAE: 0.8670, Test MAE: 0.9260\n",
      "0\n",
      "Number of parameters:  434561\n",
      "Training GCN_pair with edge at epoch 0...\n",
      "NNConv_pair(\n",
      "  (convs_d): ModuleList(\n",
      "    (0): NNConv(8, 96, aggr=\"add\", nn=Sequential(\n",
      "      (0): Linear(in_features=3, out_features=16, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=16, out_features=768, bias=True)\n",
      "    ))\n",
      "    (1): NNConv(96, 96, aggr=\"add\", nn=Sequential(\n",
      "      (0): Linear(in_features=3, out_features=16, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=16, out_features=9216, bias=True)\n",
      "    ))\n",
      "    (2): NNConv(96, 96, aggr=\"add\", nn=Sequential(\n",
      "      (0): Linear(in_features=3, out_features=16, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=16, out_features=9216, bias=True)\n",
      "    ))\n",
      "    (3): NNConv(96, 96, aggr=\"add\", nn=Sequential(\n",
      "      (0): Linear(in_features=3, out_features=16, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=16, out_features=9216, bias=True)\n",
      "    ))\n",
      "  )\n",
      "  (convs_p): ModuleList(\n",
      "    (0): NNConv(8, 96, aggr=\"add\", nn=Sequential(\n",
      "      (0): Linear(in_features=3, out_features=16, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=16, out_features=768, bias=True)\n",
      "    ))\n",
      "    (1): NNConv(96, 96, aggr=\"add\", nn=Sequential(\n",
      "      (0): Linear(in_features=3, out_features=16, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=16, out_features=9216, bias=True)\n",
      "    ))\n",
      "    (2): NNConv(96, 96, aggr=\"add\", nn=Sequential(\n",
      "      (0): Linear(in_features=3, out_features=16, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=16, out_features=9216, bias=True)\n",
      "    ))\n",
      "    (3): NNConv(96, 96, aggr=\"add\", nn=Sequential(\n",
      "      (0): Linear(in_features=3, out_features=16, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=16, out_features=9216, bias=True)\n",
      "    ))\n",
      "  )\n",
      "  (lin): ModuleList(\n",
      "    (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "    (1): Linear(in_features=192, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Training on cuda.\n",
      "Epoch: 000, Train MAE: 58.3570, Test MAE: 59.6810\n",
      "Epoch: 020, Train MAE: 1.4800, Test MAE: 1.5320\n",
      "Epoch: 040, Train MAE: 1.3670, Test MAE: 1.4360\n",
      "Epoch: 060, Train MAE: 1.1000, Test MAE: 1.1530\n",
      "Epoch: 080, Train MAE: 1.0130, Test MAE: 1.1440\n",
      "Epoch: 100, Train MAE: 0.9160, Test MAE: 0.9840\n",
      "trained/loaded gcn models successfully\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **Cross validation**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### **prepare graph and fp data for cv**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "cv_graph_data = slice_list(graph_data['train_split']+graph_data['val_split'],5)\n",
    "# cv_graph_train, cv_graph_val = data.cross_val_lists(cv_graph_data,run_cv)\n",
    "\n",
    "cv_loaders={'train':{},'val':{}}\n",
    "for i in range(5):\n",
    "    train, val = cross_val_lists(cv_graph_data,i)\n",
    "    cv_loaders['train'][i] = dataset_to_dataloader(train, BATCH_SIZE)\n",
    "    cv_loaders['val'][i] = dataset_to_dataloader(val, BATCH_SIZE)\n",
    "\n",
    "for name, nums in cv_loaders.items():\n",
    "    for num, obj in nums.items(): \n",
    "        print('graph_data: ',name, num, obj)\n",
    "        \n",
    "#create dictionary with modes as keys and a list of 5 arrays for each value\n",
    "cv_fp_data={}\n",
    "for name, array in fp_data['train_split'].items():\n",
    "    try:\n",
    "        cv_fp_data[name]=np.vstack((array,fp_data['val_split'][name]))\n",
    "    except:\n",
    "        cv_fp_data[name]=np.hstack((array,fp_data['val_split'][name]))\n",
    "for name, array in cv_fp_data.items():\n",
    "    cv_fp_data[name] = slice_list(array,5)\n",
    "\n",
    "#generate \n",
    "cv_fp_sets={'train':{},'val':{}}\n",
    "for i in range(5):\n",
    "    cv_fp_train={}\n",
    "    cv_fp_val={}\n",
    "    for name, array in cv_fp_data.items():\n",
    "        train, val = cross_val_lists(array,i)\n",
    "        cv_fp_train[name] = np.array(train, dtype=object)\n",
    "        cv_fp_val[name] = np.array(val, dtype=object)\n",
    "    cv_fp_sets['train'][i]=cv_fp_train\n",
    "    cv_fp_sets['val'][i]=cv_fp_val\n",
    "    \n",
    "for name, nums in cv_fp_sets.items():\n",
    "    for num, modes in nums.items():\n",
    "        print('fp_data: ',name, num, modes.keys())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "graph_data:  train 0 <torch_geometric.loader.dataloader.DataLoader object at 0x7f1106b84550>\n",
      "graph_data:  train 1 <torch_geometric.loader.dataloader.DataLoader object at 0x7f1106b848b0>\n",
      "graph_data:  train 2 <torch_geometric.loader.dataloader.DataLoader object at 0x7f1106b84460>\n",
      "graph_data:  train 3 <torch_geometric.loader.dataloader.DataLoader object at 0x7f1106b84520>\n",
      "graph_data:  train 4 <torch_geometric.loader.dataloader.DataLoader object at 0x7f1106a0d160>\n",
      "graph_data:  val 0 <torch_geometric.loader.dataloader.DataLoader object at 0x7f1106b84e80>\n",
      "graph_data:  val 1 <torch_geometric.loader.dataloader.DataLoader object at 0x7f1106b84220>\n",
      "graph_data:  val 2 <torch_geometric.loader.dataloader.DataLoader object at 0x7f1106b84e20>\n",
      "graph_data:  val 3 <torch_geometric.loader.dataloader.DataLoader object at 0x7f1106a0dca0>\n",
      "graph_data:  val 4 <torch_geometric.loader.dataloader.DataLoader object at 0x7f1106a0d9d0>\n",
      "fp_data:  train 0 dict_keys(['prot', 'deprot', 'pair', 'y'])\n",
      "fp_data:  train 1 dict_keys(['prot', 'deprot', 'pair', 'y'])\n",
      "fp_data:  train 2 dict_keys(['prot', 'deprot', 'pair', 'y'])\n",
      "fp_data:  train 3 dict_keys(['prot', 'deprot', 'pair', 'y'])\n",
      "fp_data:  train 4 dict_keys(['prot', 'deprot', 'pair', 'y'])\n",
      "fp_data:  val 0 dict_keys(['prot', 'deprot', 'pair', 'y'])\n",
      "fp_data:  val 1 dict_keys(['prot', 'deprot', 'pair', 'y'])\n",
      "fp_data:  val 2 dict_keys(['prot', 'deprot', 'pair', 'y'])\n",
      "fp_data:  val 3 dict_keys(['prot', 'deprot', 'pair', 'y'])\n",
      "fp_data:  val 4 dict_keys(['prot', 'deprot', 'pair', 'y'])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### **load baseline and gcn models for cv**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "mol_modes=['prot','deprot','pair']\n",
    "edge_modes=['no-edge','edge']\n",
    "cv = list(range(5))\n",
    "\n",
    "graph_models_cv = {}\n",
    "for mode in mol_modes:\n",
    "    graph_models_cv[mode] ={}\n",
    "    for edge in edge_modes:\n",
    "        graph_models_cv[mode][edge] = {}\n",
    "        for num_cv in cv: \n",
    "            path = f'cv_models/gcn/{mode}/{edge}/{num_cv}/'\n",
    "            if os.path.isfile(path+'model.pkl'):\n",
    "                with open(path+'model.pkl', 'rb') as pickle_file:\n",
    "                    graph_models_cv[mode][edge][num_cv] = pickle.load(pickle_file)\n",
    "                model = graph_models_cv[mode][edge][num_cv]\n",
    "            else:\n",
    "                print(f'cv_models/gcn/{mode}/{edge}/{num_cv}/ not found')\n",
    "\n",
    "baseline_models_cv = {}\n",
    "for name in models_dict.keys():\n",
    "    baseline_models_cv[name]={}\n",
    "    for mode in mol_modes:\n",
    "        baseline_models_cv[name][mode] ={}\n",
    "        for num_cv in cv: \n",
    "            path = f'cv_models/baseline/{name}/{mode}/{num_cv}/'\n",
    "            if os.path.isfile(path+'model.pkl'):\n",
    "                with open(path+'model.pkl', 'rb') as pickle_file:\n",
    "                    baseline_models_cv[name][mode][num_cv] = pickle.load(pickle_file)\n",
    "            else:\n",
    "                print(f'cv_models/baseline/{name}/{mode}/{num_cv}/ not found')\n",
    "                \n",
    "for name, modes in graph_models_cv.items():\n",
    "    for mode in modes.keys():\n",
    "        print('gcn: ',name, mode)\n",
    "for name, modes in baseline_models_cv.items():\n",
    "    for mode in modes.keys():\n",
    "        print('baseline: ',name, mode)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cv_models/gcn/prot/no-edge/0/ not found\n",
      "cv_models/gcn/prot/no-edge/1/ not found\n",
      "cv_models/gcn/prot/no-edge/2/ not found\n",
      "cv_models/gcn/prot/no-edge/3/ not found\n",
      "cv_models/gcn/prot/no-edge/4/ not found\n",
      "cv_models/gcn/prot/edge/0/ not found\n",
      "cv_models/gcn/prot/edge/1/ not found\n",
      "cv_models/gcn/prot/edge/2/ not found\n",
      "cv_models/gcn/prot/edge/3/ not found\n",
      "cv_models/gcn/prot/edge/4/ not found\n",
      "cv_models/gcn/deprot/no-edge/0/ not found\n",
      "cv_models/gcn/deprot/no-edge/1/ not found\n",
      "cv_models/gcn/deprot/no-edge/2/ not found\n",
      "cv_models/gcn/deprot/no-edge/3/ not found\n",
      "cv_models/gcn/deprot/no-edge/4/ not found\n",
      "cv_models/gcn/deprot/edge/0/ not found\n",
      "cv_models/gcn/deprot/edge/1/ not found\n",
      "cv_models/gcn/deprot/edge/2/ not found\n",
      "cv_models/gcn/deprot/edge/3/ not found\n",
      "cv_models/gcn/deprot/edge/4/ not found\n",
      "cv_models/gcn/pair/no-edge/0/ not found\n",
      "cv_models/gcn/pair/no-edge/1/ not found\n",
      "cv_models/gcn/pair/no-edge/2/ not found\n",
      "cv_models/gcn/pair/no-edge/3/ not found\n",
      "cv_models/gcn/pair/no-edge/4/ not found\n",
      "cv_models/gcn/pair/edge/0/ not found\n",
      "cv_models/gcn/pair/edge/1/ not found\n",
      "cv_models/gcn/pair/edge/2/ not found\n",
      "cv_models/gcn/pair/edge/3/ not found\n",
      "cv_models/gcn/pair/edge/4/ not found\n",
      "cv_models/baseline/RFR/prot/0/ not found\n",
      "cv_models/baseline/RFR/prot/1/ not found\n",
      "cv_models/baseline/RFR/prot/2/ not found\n",
      "cv_models/baseline/RFR/prot/3/ not found\n",
      "cv_models/baseline/RFR/prot/4/ not found\n",
      "cv_models/baseline/RFR/deprot/0/ not found\n",
      "cv_models/baseline/RFR/deprot/1/ not found\n",
      "cv_models/baseline/RFR/deprot/2/ not found\n",
      "cv_models/baseline/RFR/deprot/3/ not found\n",
      "cv_models/baseline/RFR/deprot/4/ not found\n",
      "cv_models/baseline/RFR/pair/0/ not found\n",
      "cv_models/baseline/RFR/pair/1/ not found\n",
      "cv_models/baseline/RFR/pair/2/ not found\n",
      "cv_models/baseline/RFR/pair/3/ not found\n",
      "cv_models/baseline/RFR/pair/4/ not found\n",
      "cv_models/baseline/PLS/prot/0/ not found\n",
      "cv_models/baseline/PLS/prot/1/ not found\n",
      "cv_models/baseline/PLS/prot/2/ not found\n",
      "cv_models/baseline/PLS/prot/3/ not found\n",
      "cv_models/baseline/PLS/prot/4/ not found\n",
      "cv_models/baseline/PLS/deprot/0/ not found\n",
      "cv_models/baseline/PLS/deprot/1/ not found\n",
      "cv_models/baseline/PLS/deprot/2/ not found\n",
      "cv_models/baseline/PLS/deprot/3/ not found\n",
      "cv_models/baseline/PLS/deprot/4/ not found\n",
      "cv_models/baseline/PLS/pair/0/ not found\n",
      "cv_models/baseline/PLS/pair/1/ not found\n",
      "cv_models/baseline/PLS/pair/2/ not found\n",
      "cv_models/baseline/PLS/pair/3/ not found\n",
      "cv_models/baseline/PLS/pair/4/ not found\n",
      "gcn:  prot no-edge\n",
      "gcn:  prot edge\n",
      "gcn:  deprot no-edge\n",
      "gcn:  deprot edge\n",
      "gcn:  pair no-edge\n",
      "gcn:  pair edge\n",
      "baseline:  RFR prot\n",
      "baseline:  RFR deprot\n",
      "baseline:  RFR pair\n",
      "baseline:  PLS prot\n",
      "baseline:  PLS deprot\n",
      "baseline:  PLS pair\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Load bestmodels**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "epoch_treshold = 2000\n",
    "mol_modes=['prot','deprot','pair']\n",
    "edge_modes=['no-edge','edge']\n",
    "# mol_modes=['pair']\n",
    "# edge_modes=['edge']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "cv = list(range(5))\n",
    "\n",
    "graph_models_cv = {}\n",
    "for mode in mol_modes:\n",
    "    graph_models_cv[mode] ={}\n",
    "    for edge in edge_modes:\n",
    "        graph_models_cv[mode][edge] = {}\n",
    "        for num_cv in cv: \n",
    "            path = f'cv_models/gcn/{mode}/{edge}/{num_cv}/'\n",
    "            if os.path.isfile(path+'model.pkl'):\n",
    "                with open(path+'model.pkl', 'rb') as pickle_file:\n",
    "                    model = pickle.load(pickle_file)\n",
    "                best_loss = max([x for x in model.checkpoint['best_states'].keys() if x < epoch_treshold]) \n",
    "                model.load_state_dict(model.checkpoint['best_states'][best_loss][1])\n",
    "                loss = model.checkpoint['best_states'][best_loss][0]\n",
    "                print(f'GCN_{mode}_{edge}_{num_cv},Epoch {best_loss}, Loss:{loss}')\n",
    "                graph_models_cv[mode][edge][num_cv] = model\n",
    "            else:\n",
    "                print(f'{path} not found')\n",
    "                "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cv_models/gcn/prot/no-edge/0/ not found\n",
      "cv_models/gcn/prot/no-edge/1/ not found\n",
      "cv_models/gcn/prot/no-edge/2/ not found\n",
      "cv_models/gcn/prot/no-edge/3/ not found\n",
      "cv_models/gcn/prot/no-edge/4/ not found\n",
      "cv_models/gcn/prot/edge/0/ not found\n",
      "cv_models/gcn/prot/edge/1/ not found\n",
      "cv_models/gcn/prot/edge/2/ not found\n",
      "cv_models/gcn/prot/edge/3/ not found\n",
      "cv_models/gcn/prot/edge/4/ not found\n",
      "cv_models/gcn/deprot/no-edge/0/ not found\n",
      "cv_models/gcn/deprot/no-edge/1/ not found\n",
      "cv_models/gcn/deprot/no-edge/2/ not found\n",
      "cv_models/gcn/deprot/no-edge/3/ not found\n",
      "cv_models/gcn/deprot/no-edge/4/ not found\n",
      "cv_models/gcn/deprot/edge/0/ not found\n",
      "cv_models/gcn/deprot/edge/1/ not found\n",
      "cv_models/gcn/deprot/edge/2/ not found\n",
      "cv_models/gcn/deprot/edge/3/ not found\n",
      "cv_models/gcn/deprot/edge/4/ not found\n",
      "cv_models/gcn/pair/no-edge/0/ not found\n",
      "cv_models/gcn/pair/no-edge/1/ not found\n",
      "cv_models/gcn/pair/no-edge/2/ not found\n",
      "cv_models/gcn/pair/no-edge/3/ not found\n",
      "cv_models/gcn/pair/no-edge/4/ not found\n",
      "cv_models/gcn/pair/edge/0/ not found\n",
      "cv_models/gcn/pair/edge/1/ not found\n",
      "cv_models/gcn/pair/edge/2/ not found\n",
      "cv_models/gcn/pair/edge/3/ not found\n",
      "cv_models/gcn/pair/edge/4/ not found\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "graph_models = {}\n",
    "for mode in mol_modes:\n",
    "    graph_models[mode] ={}\n",
    "    for edge in edge_modes:\n",
    "        graph_models[mode][edge] = {} \n",
    "        path = f'models/gcn/{mode}/{edge}/'\n",
    "        if os.path.isfile(path+'model.pkl'):\n",
    "            with open(path+'model.pkl', 'rb') as pickle_file:\n",
    "                model = pickle.load(pickle_file)\n",
    "            model.to(device='cpu')\n",
    "            best_loss = max([x for x in model.checkpoint['best_states'].keys() if x < epoch_treshold]) \n",
    "            model.load_state_dict(model.checkpoint['best_states'][best_loss][1])\n",
    "            loss = model.checkpoint['best_states'][best_loss][0]\n",
    "            print(f'GCN_{mode}_{edge},Epoch {best_loss}, Loss:{loss}')\n",
    "            graph_models[mode][edge] = model\n",
    "        else:\n",
    "            print(f'{path} not found')"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_34648/4080865046.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mbest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'best_states'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mepoch_treshold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'best_states'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'best_states'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "graph_models = {}\n",
    "for mode in mol_modes:\n",
    "    graph_models[mode] ={}\n",
    "    for edge in edge_modes:\n",
    "        graph_models[mode][edge] = {} \n",
    "        path = f'models/gcn/{mode}/{edge}/'\n",
    "        if os.path.isfile(path+'model.pkl'):\n",
    "            with open(path+'model.pkl', 'rb') as pickle_file:\n",
    "                model = pickle.load(pickle_file)\n",
    "            model.to(device='cpu')\n",
    "            # best_loss = max([x for x in model.checkpoint['best_states'].keys() if x < epoch_treshold]) \n",
    "            # model.load_state_dict(model.checkpoint['best_states'][best_loss][1])\n",
    "            # loss = model.checkpoint['best_states'][best_loss][0]\n",
    "            # print(f'GCN_{mode}_{edge},Epoch {best_loss}, Loss:{loss}')\n",
    "            graph_models[mode][edge] = model\n",
    "        else:\n",
    "            print(f'{path} not found')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Results and Analysis**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **Test Baseline and Graph Models**\n",
    "test the models and the valadation and the two external sets and store their predictions and the true valus in a DataFrame"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Predictions of baseline and graph models\n",
    "\n",
    "for i,test_set in enumerate(['Novartis', 'Literature', 'val_split']):\n",
    "# for i,test_set in enumerate(['Novartis']):\n",
    "    # df_ml = test_ml_model(baseline_models, fp_data[test_set], fp_data[test_set]['y'],test_set)\n",
    "    df_gcn = test_graph_model(graph_models, loaders[test_set],test_set)\n",
    "    # df= pd.concat([df_ml.drop(columns=['Dataset', 'pKa_true']),df_gcn],axis=1)\n",
    "    df= pd.concat([df_gcn],axis=1)\n",
    "    torch.cuda.empty_cache()\n",
    "    if i == 0:\n",
    "        df_res = df\n",
    "    else:\n",
    "        df_res = pd.concat([df_res,df])\n",
    "display(df_res)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "torch.cuda.memory_stats(device=None)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "! nvidia-smi"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **Statistical metrics**\n",
    "Calulate the Pearson Correlation Koefficient, the Root Mean Squared Error and the Mean absolute error of the validation and the two test sets for all models and list them in a DataFrame "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test= compute_stats(df_res, 'Dataset', 'pKa_true')\n",
    "test.to_csv(f'{imgdir}/stat_metrics.csv')\n",
    "display(test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test= compute_stats(df_res, 'Dataset', 'pKa_true')\n",
    "test.to_csv(f'{imgdir}/stat_metrics.csv')\n",
    "display(test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **CV results**\n",
    "Load models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cv_val_table= pd.concat((\n",
    "    stat.cv_graph_model(graph_models_cv,cv_loaders['val']),\n",
    "    stat.cv_ml_model(baseline_models_cv,cv_fp_sets['val'])\n",
    "))\n",
    "cv_val_table.to_csv(f'{imgdir}/cv_val_table.csv')\n",
    "display(cv_val_table)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "d ={}\n",
    "for data_set in ['Novartis','Literature']:\n",
    "    d[data_set]= pd.concat([\n",
    "    stat.cv_ml_model(baseline_models_cv,[fp_data[data_set] for i in range(5)]),\n",
    "    stat.cv_graph_model(graph_models_cv,[loaders[data_set] for i in range(5)])\n",
    "    ])\n",
    "cv_test_table=pd.concat(d.values(), axis=1, keys=d.keys())\n",
    "cv_test_table.to_csv(f'{imgdir}/cv_test_table.csv')\n",
    "cv_test_table"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **Plot best model**\n",
    "Plot the predictions of the best models for the validation and the two testsets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_results(df, x_column, y_column):\n",
    "    y = df[x_column]\n",
    "    y_hat = df[y_column]\n",
    "    stat_info = f\"\"\"\n",
    "        $r^2$ = {r2_score(y, y_hat): .2f}\n",
    "        $MAE$ = {mean_absolute_error(y, y_hat): .2f}\n",
    "        $RMSE$ = {mean_squared_error(y, y_hat): .2f}\n",
    "        \"\"\"\n",
    "        # r = 0.78 [0.74, 0.81]\n",
    "    g = sns.JointGrid(data=df, x=x_column, y=y_column, xlim=(2,12), ylim=(2,12), height=3.125)\n",
    "    g.plot_joint(sns.regplot)\n",
    "    g.plot_marginals(sns.kdeplot, shade=True)\n",
    "    g.ax_joint.text(0, 1, stat_info, size='x-small', ha='left', va=\"top\", transform = g.ax_joint.transAxes)\n",
    "    return g"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.set_context('paper')\n",
    "d = df_res\n",
    "model='GCN_pair_edge'\n",
    "for dataset in d['Dataset'].unique():\n",
    "# for dataset, model in zip(['Novartis','Literature'],['GCN_pair_edge', 'GCN_deprot_no-edge']):\n",
    "    print(dataset)\n",
    "    g = plot_results(d[d['Dataset']== dataset], 'pKa_true', model)\n",
    "    g.set_axis_labels('pKa (true)', 'gcn_pair_edge')\n",
    "    plt.savefig(f'{imgdir}/regression_{dataset}_gcn_pair_edge.pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(3.125,3.125))\n",
    "    sns.residplot(data=df_res[df_res['Dataset']==dataset],x='pKa_true', y=model, lowess=True)\n",
    "    plt.ylabel('Error')\n",
    "#     plt.title('gcn_pair_edge')\n",
    "    plt.savefig(f'{imgdir}/residuals_{dataset}_gcn_pair_edge.pdf', bbox_inches='tight')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **GCN training progress**\n",
    "store training losses in DataFrame and "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for x,y in list([['pair','edge'],['prot','no-edge']]):\n",
    "    df_prog=pd.DataFrame(graph_models[x][y].checkpoint['progress_table'])\n",
    "#     df_prog=pd.DataFrame(graph_models_cv[x][y][1].checkpoint['progress_table'])\n",
    "    #plot learning\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(3.125,3.125)\n",
    "    sns.lineplot(x='epoch',y='train_loss', label='Train Loss',data=df_prog,ax=ax)\n",
    "    sns.lineplot(x='epoch',y='test_loss',label='Validation Loss',data=df_prog,ax=ax)\n",
    "    ax.set_ylabel(\"Loss (MAE)\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_xlim(left=0, right=2000)\n",
    "    ax.set_ylim(top=1.75, bottom=0)\n",
    "#     plt.title(f'training progress of gcn_{x}_{y} model')\n",
    "    plt.savefig(f'{imgdir}/training_progress_gcn_{x}_{y}.pdf',bbox_inches='tight')\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **Feature impact**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Importances of gcn_prot_edge'**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def boxplot(attr_data):\n",
    "    plt.figure(figsize=(3.125,6.25))\n",
    "    sns.boxplot(x=\"value\", y=\"variable\",\n",
    "                orient=\"h\",\n",
    "                data=attr_data,\n",
    "                whis=[0, 100], width=.6)\n",
    "\n",
    "    # Add in points to show each observation\n",
    "    sns.stripplot(x=\"value\", y=\"variable\",\n",
    "                orient=\"h\",\n",
    "                data=attr_data,\n",
    "                size=4, color=\".3\", linewidth=0)\n",
    "    plt.ylabel('')\n",
    "\n",
    "types = ['prot','deprot','pair']\n",
    "f_modes= ['no-edge','edge']\n",
    "for data_type in types:\n",
    "    for f_mode in f_modes: \n",
    "        model = graph_models[data_type][f_mode]\n",
    "        dataset = graph_data['train_split']\n",
    "        ig = IntegratedGradients(model)\n",
    "        attr_pre_df = stat.calc_importances(ig, dataset, 100, NODE_FEATURES, EDGE_FEATURES) #adjust number of random samples\n",
    "\n",
    "        attr_pre_df.iloc[:, 1:]=attr_pre_df.iloc[:, 1:].abs()\n",
    "        attr_df=attr_pre_df.groupby('ID').max()\n",
    "        attr_data = pd.melt(attr_df)\n",
    "        \n",
    "        if data_type== 'pair':\n",
    "            split = len(attr_data.variable.unique())//2\n",
    "            attr_data1 = pd.melt(attr_df.iloc[:,0:split])\n",
    "            attr_data2 = pd.melt(attr_df.iloc[:,split:])\n",
    "        \n",
    "            boxplot(attr_data1)\n",
    "#             plt.title(f'gcn_{data_type}_1_{f_mode}')\n",
    "            plt.savefig(f'{imgdir}/importances_{data_type}_1_{f_mode}.pdf', bbox_inches='tight')\n",
    "            plt.show()\n",
    "            boxplot(attr_data2)\n",
    "#             plt.title(f'gcn_{data_type}_2_{f_mode}')\n",
    "            plt.savefig(f'{imgdir}/importances_{data_type}_2_{f_mode}.pdf', bbox_inches='tight')\n",
    "            plt.show()\n",
    "            \n",
    "        else:\n",
    "            boxplot(attr_data)\n",
    "#             plt.title(f'gcn_{data_type}_{f_mode}')\n",
    "            plt.savefig(f'{imgdir}/importances_{data_type}_{f_mode}.pdf', bbox_inches='tight')\n",
    "            plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Metrics by tanimoto similarity**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x1= pd.concat([df_res[['Dataset', 'pKa_true']],df_res.loc[:,(df_res.columns.str.startswith('GCN'))]],axis=1)\n",
    "for data_set in ['Novartis', 'Literature']:\n",
    "    df = x1[x1['Dataset']==data_set].copy()\n",
    "    df['similarity'] = data_dfs[data_set].loc[:,'Similarity_max']\n",
    "    df.sort_values(by=['similarity'], inplace=True)\n",
    "\n",
    "    res=[]\n",
    "    tanimoto=[]\n",
    "    maximum=0\n",
    "\n",
    "    for i in range(2,len(df)):\n",
    "        df2 = df.iloc[:i,:]\n",
    "        new_maximum = df2['similarity'].max()\n",
    "        if new_maximum <= maximum:\n",
    "            tanimoto[-1]= new_maximum\n",
    "            res[-1]= stat.compute_stats(df2, 'Dataset', 'pKa_true',col_exclude=['similarity'])\n",
    "        else: \n",
    "            tanimoto.append(new_maximum)\n",
    "            res.append(stat.compute_stats(df2, 'Dataset', 'pKa_true', col_exclude=['similarity']))\n",
    "        maximum=new_maximum\n",
    "    result = pd.concat((res), keys=tanimoto)\n",
    "    result\n",
    "\n",
    "    # X=result['Novartis'].loc[(slice(None),'pKa_gcn_prot_edge'),:].reset_index()\n",
    "    X=result[data_set].reset_index()\n",
    "    plt.figure(figsize=(6.25,4))\n",
    "    ax = sns.scatterplot(x='level_0', y=\"RMSE\", hue='level_1', palette='colorblind', data=X)\n",
    "    legend = ax.legend()\n",
    "    legend.texts[0] = ''\n",
    "    plt.xlabel('Similarity')\n",
    "    plt.savefig(f'{imgdir}/RMSE_sim_{data_set}.pdf')\n",
    "    \n",
    "    x2= x1\n",
    "    df = x2[x2['Dataset']==data_set].copy()\n",
    "    df['similarity'] = data_dfs[data_set].loc[:,'Similarity_max']\n",
    "    df.sort_values(by=['similarity'], inplace=True)\n",
    "    \n",
    "    df = df.loc[:,('pKa_true','GCN_pair_edge','similarity')]\n",
    "\n",
    "    df['Error']= df['GCN_pair_edge']-df['pKa_true']\n",
    "\n",
    "    sims=[]\n",
    "    step_size=0.15\n",
    "    for sim in df['similarity']:\n",
    "        x=1\n",
    "        while sim < x:\n",
    "            x+= -step_size\n",
    "        sims.append(f'< {round(np.clip(x+step_size,0,1),3)}')\n",
    "    df['group']=sims            \n",
    "\n",
    "    plt.figure(figsize=(6.25/2,2.5))\n",
    "    sns.boxplot(x=\"group\", y=\"Error\",\n",
    "                orient=\"v\",\n",
    "\n",
    "                whis=[0, 100], width=.6,\n",
    "                data=df\n",
    "               )\n",
    "    # Add in points to show each observation\n",
    "    sns.stripplot(x=\"group\", y=\"Error\",\n",
    "                orient=\"v\",\n",
    "                data=df,\n",
    "                  size=4, color=\".3\", linewidth=0)\n",
    "    plt.xlabel('Similarity')\n",
    "    plt.ylabel('Error [pKa units]')\n",
    "    plt.savefig(f'{imgdir}/error_sim_bloxplot_{data_set}.pdf', bbox_inches='tight')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Outliers top list**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_2d_molecule(m):\n",
    "  copy = Chem.Mol(m)\n",
    "  copy.Compute2DCoords(clearConfs=True)\n",
    "  return copy\n",
    "\n",
    "def group_by_range(series, range_list, decimal=1):\n",
    "    group_labels=[]\n",
    "    for x in series:\n",
    "        i=0\n",
    "        while x > range_list[i]:\n",
    "            i+=1\n",
    "        group_labels.append(round(range_list[i],decimal))\n",
    "    return group_labels \n",
    "\n",
    "data_set=['Novartis', 'Literature']\n",
    "best=[True, False]\n",
    "for data_set in data_set:\n",
    "    trues= df_res[df_res.Dataset==data_set].pKa_true\n",
    "    preds= df_res[df_res.Dataset==data_set].GCN_pair_edge\n",
    "    diffs = []\n",
    "    errors = []\n",
    "    for pred, true in zip(preds,trues):\n",
    "        diffs.append(pred-true)\n",
    "        errors.append(abs(pred-true))\n",
    "    res = pd.concat((pd.DataFrame({'differences':diffs}),pd.DataFrame({'errors':errors}), data_dfs['Novartis'].loc[:,('pKa','marvin_atom','protonated', 'deprotonated', 'ID','Similarity_max')]),axis=1)\n",
    "    \n",
    "    res_e=res.loc[:, ('errors','pKa','Similarity_max')]\n",
    "    res_e['pKa']=group_by_range(res_e['pKa'],list(range(2,14,2)))\n",
    "    res_e['Similarity']=group_by_range(res['Similarity_max'],np.arange(0.0,1.2,0.2))\n",
    "    res_e=res_e.loc[:, ('errors','pKa','Similarity')]\n",
    "    res_e=res_e.groupby(['Similarity','pKa']).mean().unstack()\n",
    "#     display(res_e)\n",
    "    \n",
    "    plt.figure(figsize=(3.125,2.5))\n",
    "    sns.heatmap(res_e['errors'], cmap='RdYlGn_r', vmin=0,vmax=1.50)\n",
    "    plt.savefig(f'{imgdir}/error_heatmap_{data_set}.pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    for mod in best:    \n",
    "        res.sort_values(by=['errors'], inplace=True, ascending=mod)\n",
    "        num=6\n",
    "        img=Draw.MolsToGridImage(res.protonated[:num].map(get_2d_molecule),\n",
    "                                 molsPerRow=3,\n",
    "                                 subImgSize=(400,350),\n",
    "                                 useSVG=True,\n",
    "                                 highlightAtomLists=[[int(i)] for i in res.marvin_atom[:num]],\n",
    "                                 legends=[f\"error:  {round(x[1],2)}, pKa:  {x[0]}, sim: {x[2]:.3f}\" for x in zip(res.pKa[:num],res.differences[:num], res.Similarity_max[:num])])\n",
    "\n",
    "        display(img)\n",
    "        name_dict={True:'best',False:'outlier'}\n",
    "        with open(f'{imgdir}/grid_{data_set}_{name_dict[mod]}.svg', 'w') as f:\n",
    "            f.write(img.data)\n",
    "    # res.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1434fa83986e896e0019747847b0aafc5ed3dae518502525b9f0867561471b0e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}